{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "357ec685",
   "metadata": {},
   "outputs": [],
   "source": [
    " %%capture\n",
    "#!pip install wget\n",
    "#!pip install torchsummaryX\n",
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c0b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a149862",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_407838/4091658419.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import wget\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torchsummaryX import summary\n",
    "from torch.optim import lr_scheduler\n",
    "from IPython.core.display import display, HTML\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from datetime import datetime\n",
    "import time\n",
    "from os.path import exists\n",
    "now = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db597cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wget https://github.com/Toloka/WSDMCup2023/raw/main/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda4f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/home/ngoc/data/WSDM2023/\"\n",
    "TRAIN_IMGS_DIR = DATA_DIR + \"train_imgs/\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 4                                        \n",
    "NUM_EPOCHS = 50                   \n",
    "EMBEDDIND_SIZE = 1024            \n",
    "WORD_EMBEDING_SIZE = 300                     \n",
    "NUMBER_LSTM_LAYER = 1                                            \n",
    "LEARNING_RATE = 0.00001                                                               \n",
    "IMAGE_SIZE = 640                                   \n",
    "LOSS = \"RMSLE\"\n",
    "SAVE_IMAGE_FREQ = int(NUM_EPOCHS/5)\n",
    "LSTM_HIDDEN_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1a3908",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TRAINING  = True\n",
    "LOG_WANDB = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735ce3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if LOG_WANDB:\n",
    "    import wandb\n",
    "    wandb.init(project=\"WSDM2023\",\n",
    "               entity=\"ngocdunghuynh\",\n",
    "                config = {\n",
    "                    \"learning_rate\": LEARNING_RATE,\n",
    "                    \"epochs\": NUM_EPOCHS,\n",
    "                    \"batch_size\": BATCH_SIZE,\n",
    "                    \"device\": DEVICE,\n",
    "                    \"loss_function\": LOSS,\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af0da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR  + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "try:\n",
    "    os.mkdir(TRAIN_IMGS_DIR)\n",
    "    img_paths = Parallel(\n",
    "    n_jobs=100)(delayed(wget.download)(img_url, out= TRAIN_IMGS_DIR) for img_url in tqdm(train.image)\n",
    "    )\n",
    "except:\n",
    "    print(\"the training set has been downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cd214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the image path from URL\n",
    "train.image  = train.image.apply(lambda x: str(x.split(\"/\")[-1]))\n",
    "train.image  = train.image.apply(lambda x: TRAIN_IMGS_DIR + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e64c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spit the original training set into 2 dataframes\n",
    "train_sample = int(len(train)*0.9)\n",
    "df_train = train[:int(len(train)*0.9)]\n",
    "df_val = train[int(len(train)*0.9):]\n",
    "\n",
    "df_train = df_train.reset_index(drop =True)\n",
    "df_val = df_val.reset_index(drop =True)\n",
    "\n",
    "if TEST_TRAINING:\n",
    "    df_train = df_train[:BATCH_SIZE*100]\n",
    "    df_val = df_val[:BATCH_SIZE*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45278a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "\n",
    "        \"\"\"Split string lines into lists\"\"\"\n",
    "\n",
    "        SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n",
    "        tokens = SENTENCE_SPLIT_REGEX.split(sentence.lower())\n",
    "        tokens = [t.strip() for t in tokens if len(t.strip()) > 0]\n",
    "        return tokens\n",
    "        \n",
    "class Text_Dict:\n",
    "\n",
    "    \"\"\"Aim to convert index to word or word to index\"\"\"\n",
    "\n",
    "    def __init__(self, vocab):\n",
    "        \n",
    "        self.word_list = vocab\n",
    "        self.word2idx_dict = {w:n_w for n_w, w in enumerate(self.word_list)}\n",
    "        self.vocab_size = len(self.word_list)\n",
    "        self.unk2idx = self.word2idx_dict['<unk>'] if '<unk>' in self.word2idx_dict else None\n",
    "\n",
    "    def idx2word(self, n_w):\n",
    "\n",
    "        return self.word_list[n_w]\n",
    "\n",
    "    def word2idx(self, w):\n",
    "        if w in self.word2idx_dict:\n",
    "            return self.word2idx_dict[w]\n",
    "        elif self.unk2idx is not None:\n",
    "            return self.unk2idx\n",
    "        else:\n",
    "            raise ValueError(f'word {w} not in dictionary (while dictionary does not contain <unk>)')\n",
    "\n",
    "    def tokenize_and_index(self, sentence):\n",
    "        inds = [self.word2idx(w) for w in tokenize(sentence)]\n",
    "\n",
    "        return inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca32d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Loaders:\n",
    "\n",
    "    \"\"\"Aim to convert index to word or word to index\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, df_train, df_val, batch_size, img_size, normalised_box):\n",
    "        self.img_size = img_size\n",
    "        self.df_train = df_train\n",
    "        self.df_val = df_val\n",
    "        self.batch_size = batch_size\n",
    "        self.question_vocab, self.max_question_len = self.make_vocab_questions(pd.concat([df_train[\"question\"],df_val[\"question\"]], axis = 0))\n",
    "        self.question_vocab_size = len(self.question_vocab)\n",
    "        self.question_dict = Text_Dict(self.question_vocab)\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        self.data_dir = data_dir\n",
    "        self.normalised_box  = normalised_box\n",
    "        os.system(f'mkdir {self.data_dir + \"resized_imgs\"}')\n",
    "        os.system(f'mkdir {self.data_dir + \"resized_boxes\"}')\n",
    "        os.system(f'mkdir {self.data_dir + \"processed_boxes\"}')\n",
    "        os.system(f'saved_images')\n",
    "        #self.transform = transforms.Compose([transforms.ToTensor(),\n",
    "        #                                     transforms.Normalize((0.485, 0.456, 0.406),\\\n",
    "        #                                                          (0.229, 0.224, 0.225))])\n",
    "     #Image and BBox Processing \n",
    "     #                                                             \n",
    "    def create_mask(self, bb, x):\n",
    "        \"\"\"Creates a mask for the bounding box of same shape as image\"\"\"\n",
    "        cols, rows,*_ = x.shape\n",
    "        Y = np.zeros((cols, rows))\n",
    "        bb = bb.astype(np.int64)\n",
    "        Y[bb[1]:bb[3], bb[0]:bb[2]] = 1.\n",
    "        return Y\n",
    "\n",
    "    def mask_to_bb(self, Y):\n",
    "\n",
    "        \"\"\"Convert mask Y to a bounding box, assumes 0 as background nonzero object\"\"\"\n",
    "\n",
    "        rows , cols = np.nonzero(Y)\n",
    "        if len(cols)==0: \n",
    "            return np.zeros(4, dtype=np.float64)\n",
    "        top_row = np.min(rows)\n",
    "        left_col = np.min(cols)\n",
    "        bottom_row = np.max(rows)\n",
    "        right_col = np.max(cols)\n",
    "        return np.array([left_col, top_row, right_col, bottom_row], dtype=np.int64)\n",
    "\n",
    "    def xyxy2xywh(self, box):\n",
    "\n",
    "        \"\"\"Normalise box from xyXY -> xywh\"\"\"\n",
    "\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        w = xmax - xmin         # Width of the box\n",
    "        h = ymax - ymin         # Height of the box\n",
    "        x_center = (xmin + (w/2))/self.img_size \n",
    "        y_center = (ymin + (h/2))/self.img_size \n",
    "        new_w = w/self.img_size \n",
    "        new_h = h/self.img_size\n",
    "        return np.array([x_center,y_center, new_w, new_h], dtype=np.float64)\n",
    "\n",
    "    def resize_image_bb(self, img_path, bb, sz):\n",
    "\n",
    "        \"\"\"Resize an image and its bounding box and write image to new path\"\"\"\n",
    "        img = cv2.cvtColor(cv2.imread(str(img_path)), cv2.COLOR_BGR2RGB)\n",
    "        mask = self.create_mask(bb, img)\n",
    "        img_resized = cv2.resize(img, (sz, sz))\n",
    "        mask_resized = cv2.resize(mask, (sz, sz))\n",
    "        bb_resized = self.mask_to_bb(mask_resized)\n",
    "        processed_box = self.xyxy2xywh(bb_resized)\n",
    "        return img_resized, bb_resized, processed_box\n",
    "    \n",
    "    #Question Proxessing\n",
    "    def make_vocab_questions(self, questions):\n",
    "\n",
    "        \"\"\"Make dictionary for questions and save them into text file.\"\"\"\n",
    "\n",
    "        vocab_set = set()\n",
    "        SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n",
    "        question_length = []\n",
    "        set_question_length = [None]*len(questions)\n",
    "        for iquestion, question in enumerate(questions):\n",
    "            words = SENTENCE_SPLIT_REGEX.split(question.lower())\n",
    "            words = [w.strip() for w in words if len(w.strip()) > 0]\n",
    "            vocab_set.update(words)\n",
    "            set_question_length[iquestion] = len(words)\n",
    "        question_length += set_question_length\n",
    "\n",
    "        vocab_list = list(vocab_set)\n",
    "        vocab_list.sort()\n",
    "        vocab_list.insert(0, '<pad>')\n",
    "        vocab_list.insert(1, '<unk>')\n",
    "        return vocab_list, np.max(question_length)\n",
    "\n",
    "    #data batch processing\n",
    "    def vqa_processing(self,df):\n",
    "        \"\"\"Create a data list to store all raw data simples\"\"\"\n",
    "        data = []\n",
    "        for idx in range(len(df)):\n",
    "            image_path, _, _, l, t, r,b, question = df.iloc[idx]\n",
    "            data.append({\n",
    "                \"image_path\" : image_path,\n",
    "                \"text_question\": question,\n",
    "                \"bbox\": np.array([l, t, r,b], dtype =np.float64) #Xmin Ymin Xmax Ymax\n",
    "            })\n",
    "        return data\n",
    "\n",
    "    def data_processing(self, data, question_dict, transform):\n",
    "        images = []\n",
    "        text_questions = []\n",
    "        token_questions = []\n",
    "        bboxes = []\n",
    "        image_paths = []\n",
    "        for sample in data:\n",
    "            file_name = sample[\"image_path\"].split('/')[-1][:-4]\n",
    "            if exists(self.data_dir + \"resized_imgs/\" + file_name + '.npy'):\n",
    "                resized_image = np.load(self.data_dir + \"resized_imgs/\" + file_name + '.npy')\n",
    "                bb_resized = np.load(self.data_dir + \"resized_boxes/\" + file_name + '.npy')\n",
    "                processed_bb = np.load(self.data_dir + \"processed_boxes/\" + file_name + '.npy')\n",
    "            else:\n",
    "                resized_image,  bb_resized , processed_bb = self.resize_image_bb(sample[\"image_path\"], sample[\"bbox\"], self.img_size)\n",
    "                np.save(self.data_dir + \"resized_imgs/\" + file_name + '.npy', resized_image)\n",
    "                np.save(self.data_dir + \"resized_boxes/\" + file_name + '.npy', bb_resized)\n",
    "                np.save(self.data_dir + \"processed_boxes/\" + file_name + '.npy', processed_bb)\n",
    "                \n",
    "            token_question = tokenize(sample[\"text_question\"])\n",
    "            question_2_idx = np.array([question_dict.word2idx(w) for w in token_question], dtype = int)\n",
    "            question_2_idx = torch.tensor(question_2_idx)\n",
    "            images.append(transform(resized_image))\n",
    "            text_questions.append(sample[\"text_question\"])\n",
    "            token_questions.append(question_2_idx)\n",
    "            if self.normalised_box:\n",
    "                bboxes.append(processed_bb)\n",
    "            else:\n",
    "                bboxes.append(bb_resized)\n",
    "            image_paths.append(sample[\"image_path\"])\n",
    "            \n",
    "        images = np.array([np.array(image) for image in images], dtype = np.float64)\n",
    "        bboxes = np.array(bboxes, dtype = np.float64)\n",
    "        token_questions = nn.utils.rnn.pad_sequence(token_questions, padding_value = 0, batch_first =True)\n",
    "        return {\n",
    "            \"image\":torch.tensor(images),\n",
    "            \"question\": token_questions,\n",
    "            \"text_question\": text_questions,\n",
    "            \"bbox\": torch.tensor(bboxes),\n",
    "            \"image_path\": image_paths\n",
    "        }\n",
    "\n",
    "    def train_loader(self):\n",
    "        train_data = self.vqa_processing(self.df_train)\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset=train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=lambda x: self.data_processing(x, self.question_dict, self.transform))\n",
    "        return train_loader\n",
    "\n",
    "    def val_loader(self):\n",
    "        train_data = self.vqa_processing(self.df_val)\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset=train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            collate_fn=lambda x: self.data_processing(x, self.question_dict, self.transform))\n",
    "        return train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659bf849",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e080df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size):\n",
    "        \"\"\"(1) Load the pretrained model as you want.\n",
    "               cf) one needs to check structure of model using 'print(model)'\n",
    "                   to remove last fc layer from the model.\n",
    "           (2) Replace final fc layer (score values from the ImageNet)\n",
    "               with new fc layer (image feature).\n",
    "           (3) Normalize feature vector.\n",
    "        \"\"\"\n",
    "        super(ImgEncoder, self).__init__()\n",
    "        self.cnn = models.vgg16(weights='VGG16_Weights.IMAGENET1K_V1').features\n",
    "        self.fc = nn.Sequential(nn.Linear(self.cnn[-3].out_channels, embedding_size),\n",
    "                                nn.Tanh())\n",
    "\n",
    "\n",
    "    def forward(self, image):\n",
    "        \"\"\"Extract feature vector from image vector.\n",
    "    #     \"\"\"\n",
    "        with torch.no_grad():\n",
    "            img_feature = self.cnn(image)\n",
    "        img_feature = img_feature.view(-1, 512, 20*20).transpose(1,2)\n",
    "        img_feature = self.fc(img_feature)\n",
    "        return img_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecccbfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QstEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, qst_vocab_size, word_embed_size, embed_size, num_layers, hidden_size):\n",
    "\n",
    "        super(QstEncoder, self).__init__()\n",
    "        self.word2vec = nn.Embedding(qst_vocab_size, word_embed_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(word_embed_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, question):\n",
    "\n",
    "        qst_vec = self.word2vec(question)                             # [batch_size, max_qst_length, word_embed_size]                            # [max_qst_length, batch_size, word_embed_size]\n",
    "        _, (hidden, cell) = self.lstm(qst_vec)                        # [num_layers, batch_size, hidden_size]                   # [num_layers, batch_size, 2*hidden_size]\n",
    "        return hidden[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d192c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, k, d, dropout=True):\n",
    "        \"\"\"Stacked attention Module\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self.ff_image = nn.Linear(d, k)\n",
    "        self.ff_questions = nn.Linear(d, k)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.ff_attention = nn.Linear(k, 1)\n",
    "\n",
    "    def forward(self, vi, vq):\n",
    "        \"\"\"Extract feature vector from image vector.\n",
    "        \"\"\"\n",
    "        # [Batch_size, Number of image regions, Representation Dimention] -> # [Batch_size, Number of image regions, k]\n",
    "        hi = self.ff_image(vi)\n",
    "\n",
    "        # [Batch_size, Representation Dimention] -> # [Batch_size, 1, k]\n",
    "        hq = self.ff_questions(vq).unsqueeze(dim=1)\n",
    "\n",
    "        # [Batch_size, Number of image regions,  k]\n",
    "        ha = torch.tanh(hi+hq)\n",
    "        if self.dropout:\n",
    "            ha = self.dropout(ha)\n",
    "        \n",
    "        # Linear Attention\n",
    "        # [Batch_size, Number of image regions,  k] -> [Batch_size, Number of image regions,  k]\n",
    "        ha = self.ff_attention(ha)\n",
    "        # [Batch_size, Number of image regions,  k] -> N * 196\n",
    "        pi = torch.softmax(ha, dim=1)\n",
    "        self.pi = pi\n",
    "        vi_attended = (pi * vi).sum(dim=1)\n",
    "        u = vi_attended + vq\n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970669ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VqaModel(nn.Module):\n",
    "\n",
    "    def __init__(self,embedding_size, question_vocab_size, word_embedding_size, hidden_size, num_layers, num_att_layers):\n",
    "        \"\"\"\n",
    "        Fusing Image feature and question feature using Full Connected Layer\n",
    "        \"\"\"\n",
    "        super(VqaModel, self).__init__()\n",
    "        self.image_encoder =  ImgEncoder(embedding_size)\n",
    "        self.question_encoder = QstEncoder(question_vocab_size, word_embedding_size, embedding_size,num_layers, hidden_size)\n",
    "        self.Stacked_Attend = nn.ModuleList(\n",
    "            [Attention(k=512, d=embedding_size)] * num_att_layers)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(embedding_size, 4))\n",
    "\n",
    "    def forward(self, image, question):\n",
    "        img_feature = self.image_encoder(image)\n",
    "        qst_feature = self.question_encoder(question)\n",
    "        vi = img_feature\n",
    "        u = qst_feature\n",
    "        for attn_layer in self.Stacked_Attend:\n",
    "            u = attn_layer(vi, u)\n",
    "        combined_feature = self.mlp(u) \n",
    "        return combined_feature.sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394b3361",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f818ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh_to_xyxy_batch(boxes1, boxes2, img_size):\n",
    "    boxes1 = boxes1*img_size\n",
    "    boxes2 = boxes2*img_size\n",
    "    boxes1[:,0], boxes1[:,2] = boxes1[:, 0] - boxes1[:, 2] / 2, boxes1[:, 0] + boxes1[:, 2] / 2\n",
    "    boxes1[:,1], boxes1[:,3] = boxes1[:, 1] - boxes1[:, 3] / 2, boxes1[:, 1] + boxes1[:, 3] / 2\n",
    "    boxes2[:,0], boxes2[:,2] = boxes2[:, 0] - boxes2[:, 2] / 2, boxes2[:, 0] + boxes2[:, 2] / 2\n",
    "    boxes2[:,1], boxes2[:,3] = boxes2[:, 1] - boxes2[:, 3] / 2, boxes2[:, 1] + boxes2[:, 3] / 2\n",
    "    return boxes1, boxes2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13d8cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbox_iou(boxes1, boxes2, img_size, xyxy = False):\n",
    "    \"\"\"\n",
    "    Returns the IoU of two bounding boxes\n",
    "    \"\"\"\n",
    "    if not xyxy:\n",
    "        boxes1, boxes2 = xywh_to_xyxy_batch(boxes1, boxes2, img_size)\n",
    "    # Get the coordinates of bounding boxes\n",
    "    b1_x1, b1_y1, b1_x2, b1_y2 = boxes1[:, 0], boxes1[:, 1], boxes1[:, 2], boxes1[:, 3]\n",
    "    b2_x1, b2_y1, b2_x2, b2_y2 = boxes2[:, 0], boxes2[:, 1], boxes2[:, 2], boxes2[:, 3]\n",
    "\n",
    "    # get the coordinates of the intersection rectangle\n",
    "    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n",
    "    # Intersection area\n",
    "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1, 0) * torch.clamp(inter_rect_y2 - inter_rect_y1, 0)\n",
    "    # Union Area\n",
    "    b1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)\n",
    "    b2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)\n",
    "    \n",
    "    ious = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
    "    return ious\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45d8196",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "    def forward(self, pred, actual):\n",
    "        loss = torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259f6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction(img_path, pred_bb, gt_bb, question, epoch):\n",
    "    img = cv2.cvtColor(cv2.imread(str(img_path)), cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    result = img.copy()\n",
    "    cv2.rectangle(result, (int(gt_bb[0]), int(gt_bb[1])), (int(gt_bb[2]), int(gt_bb[3])), (0, 255, 0), 3)\n",
    "    cv2.rectangle(result, (int(pred_bb[0]), int(pred_bb[1])), (int(pred_bb[2]), int(pred_bb[3])), (255, 0, 0), 3)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(question)\n",
    "    plt.savefig(f\"./saved_images/epoch_{epoch}_\" + img_path.split(\"/\")[-1])\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a7f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(epoch, data_loaders, model, loss_fn, optimizer, train_table):\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_iou = 0.0\n",
    "    train_loader = data_loaders.train_loader()\n",
    "    batch_size = data_loaders.batch_size\n",
    "    size = len( data_loaders.df_train)\n",
    "    num_batch =  size/batch_size\n",
    "    for batch_idx, batch_data in enumerate(train_loader):\n",
    "        images, questions = batch_data[\"image\"].float().to(DEVICE), batch_data[\"question\"].to(DEVICE)\n",
    "        targets = batch_data[\"bbox\"].float().to(DEVICE)\n",
    "        \n",
    "        # Compute prediction and loss\n",
    "        preds = model(images, questions)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        running_loss +=  loss.item()\n",
    "\n",
    "        #Cal ious\n",
    "        if data_loaders.normalised_box:\n",
    "            ious = bbox_iou(preds, targets, data_loaders.img_size)\n",
    "        else:\n",
    "            ious = bbox_iou(preds, targets, data_loaders.img_size, xyxy=True)\n",
    "        mean_iou = torch.mean(ious).item()\n",
    "\n",
    "        running_iou += mean_iou\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #WandB log\n",
    "        if LOG_WANDB:\n",
    "            wandb.log({\n",
    "                \"loss_step\": loss.item(),\n",
    "                \"iou_step\": mean_iou\n",
    "            })\n",
    "        if (batch_idx +1) % 200 == 0:\n",
    "            print(f'    [{(batch_idx+1)*batch_size}/{size}] | Loss : {loss.item():>7f} | IoU: {mean_iou:>5f}')\n",
    "    running_loss = running_loss/num_batch\n",
    "    running_iou = running_iou/num_batch\n",
    "\n",
    "    #WandB log\n",
    "    if LOG_WANDB:\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch+1,\n",
    "            \"loss\": running_loss,\n",
    "            \"iou\": running_iou\n",
    "        })\n",
    "        if (epoch + 1) % SAVE_IMAGE_FREQ == 0:\n",
    "            if data_loaders.normalised_box:\n",
    "                preds, targets = xywh_to_xyxy_batch(preds, targets, data_loaders.img_size)\n",
    "            for idx in range(len(targets)):\n",
    "                img_path = batch_data['image_path'][idx]\n",
    "                ques_text = batch_data['text_question'][idx]\n",
    "                target = targets[idx]\n",
    "                prediction = preds[idx]\n",
    "                save_prediction(img_path, prediction, target, ques_text, epoch+1)\n",
    "                train_table.add_data(epoch+1, \"train\", wandb.Image(f\"./saved_images/epoch_{epoch+1}_\" + img_path.split(\"/\")[-1]), ious[idx].item())\n",
    "            \n",
    "    print(f'\\nTRAINING | Loss: {running_loss:>7f} | IoU: {running_iou:>5f} | Running_time : {time.time() - start_time:>5f}s')\n",
    "    return running_loss, running_iou, train_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849f2124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop(epoch, data_loaders, model, loss_fn, val_table):\n",
    "    start_time = time.time()\n",
    "    running_loss = 0.0\n",
    "    running_iou = 0.0\n",
    "    val_loader = data_loaders.val_loader()\n",
    "    batch_size = data_loaders.batch_size\n",
    "    num_batch =  len( data_loaders.df_val)/batch_size\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch_data in enumerate(val_loader):\n",
    "            images, questions = batch_data[\"image\"].float().to(DEVICE), batch_data[\"question\"].to(DEVICE)\n",
    "            targets = batch_data[\"bbox\"].float().to(DEVICE)\n",
    "            # Compute prediction and loss\n",
    "            preds = model(images, questions)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            running_loss +=  loss.item()\n",
    "\n",
    "            #Cal ious\n",
    "            if data_loaders.normalised_box:\n",
    "                ious = bbox_iou(preds, targets, data_loaders.img_size)\n",
    "            else:\n",
    "                ious = bbox_iou(preds, targets, data_loaders.img_size, xyxy=True)\n",
    "                \n",
    "            running_iou += torch.mean(ious).item()\n",
    "            \n",
    "    running_loss = running_loss/num_batch\n",
    "    running_iou = running_iou/num_batch\n",
    "    print(f'VAL      | Loss: {running_loss:>7f} | IoU: {running_iou:>5f} | Running_time : {time.time() - start_time:>5f}s')\n",
    "\n",
    "    if LOG_WANDB:\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch+1,\n",
    "            \"val_loss\":  running_loss,\n",
    "            \"val_iou\": running_iou\n",
    "        })\n",
    "        if (epoch + 1) % SAVE_IMAGE_FREQ == 0:\n",
    "            if data_loaders.normalised_box:\n",
    "                preds, targets = xywh_to_xyxy_batch(preds, targets, data_loaders.img_size)\n",
    "            for idx in range(len(targets)):\n",
    "                img_path = batch_data['image_path'][idx]\n",
    "                ques_text = batch_data['text_question'][idx]\n",
    "                target = targets[idx]\n",
    "                prediction = preds[idx]\n",
    "                save_prediction(img_path, prediction, target, ques_text, epoch+1)\n",
    "                val_table.add_data(epoch+1, \"val\", wandb.Image(f\"./saved_images/epoch_{epoch+1}_\" + img_path.split(\"/\")[-1]), ious[idx].item())\n",
    "    return running_loss, running_iou, val_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8da4fbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "data_loaders = Data_Loaders(DATA_DIR, df_train, df_val, BATCH_SIZE, IMAGE_SIZE, normalised_box=True)\n",
    "\n",
    "print('create the model')\n",
    "model = VqaModel(\n",
    "    embedding_size = EMBEDDIND_SIZE,\n",
    "    question_vocab_size = data_loaders.question_vocab_size,\n",
    "    word_embedding_size = WORD_EMBEDING_SIZE,\n",
    "    hidden_size=LSTM_HIDDEN_SIZE,\n",
    "    num_layers=NUMBER_LSTM_LAYER,\n",
    "    num_att_layers=3 \n",
    ").to(DEVICE)\n",
    " \n",
    "params = model.parameters()\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=LEARNING_RATE)\n",
    "loss_fn = RMSLELoss()  #or nn.MSELoss(reduction='mean')\n",
    "\n",
    "max_iou = 0\n",
    "if LOG_WANDB:\n",
    "    os.system('mkdir saved_images')\n",
    "    columns=[\"epoch\", \"dataset\", \"image\", \"iou\"]\n",
    "    train_table = wandb.Table(columns=columns)\n",
    "    val_table = wandb.Table(columns=columns)\n",
    "else:\n",
    "    train_table = None\n",
    "    val_table = None\n",
    "logs = {\n",
    "        \"loss\": [],\n",
    "        \"iou\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_iou\": []\n",
    "        }\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"------------------------------------------------------------------------------------------------\")\n",
    "    print('------------------------------------------------------------------------------------------------')\n",
    "    print(f\"EPOCH {epoch+1}\\n\")\n",
    "\n",
    "    loss, iou, train_table = train_loop(epoch, data_loaders, model, loss_fn, optimizer, train_table)\n",
    "    val_loss, val_iou, val_table= val_loop(epoch, data_loaders, model, loss_fn, val_table)\n",
    "    if val_iou > max_iou:\n",
    "        max_iou = val_iou\n",
    "        torch.save(model, 'saved_models/model.pt')\n",
    "    \n",
    "    logs[\"loss\"].append(loss)\n",
    "    logs[\"iou\"].append(iou)\n",
    "    logs[\"val_loss\"].append(val_loss)\n",
    "    logs[\"val_iou\"].append(val_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51941c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(logs[\"loss\"], label='train')\n",
    "plt.plot(logs[\"val_loss\"], label='val')\n",
    "plt.title(\"Loss\")\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSLE')\n",
    "\n",
    "plt.subplot(1, 2, 2) # index 2\n",
    "plt.plot(logs[\"iou\"], label='train')\n",
    "plt.plot(logs[\"val_iou\"], label='val')\n",
    "plt.title(\"IoU\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel('IoU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dc2d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_WANDB:\n",
    "    wandb.log({\"train example\": train_table})\n",
    "    wandb.log({\"val example\": val_table})\n",
    "    os.system('rm -rf saved_images')\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f9e46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb674114faabae68cf36104dc7fe3e83bd3adcaaa054f3409c3c57bd6a2f8498"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
