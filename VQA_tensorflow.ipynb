{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "815e24b6",
   "metadata": {
    "id": "815e24b6"
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "458e45c7",
   "metadata": {
    "id": "458e45c7"
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "#!pip install wget\n",
    "#!pip install wandb\n",
    "#!pip install plot_keras_history\n",
    "#!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cadab368",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cadab368",
    "outputId": "cf47cb72-b452-4e04-ac8c-0a3ed724c340"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/ngoc/anaconda3/envs/py37/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n",
      "mkdir: cannot create directory ‘saved_images’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "!mkdir saved_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bb9c2de",
   "metadata": {
    "id": "3bb9c2de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 19:18:17.546756: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import wget\n",
    "import shutil\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tqdm.auto import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from plot_keras_history import show_history, plot_history\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "from tensorflow import keras\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import random\n",
    "from wandb.keras import WandbCallback\n",
    "import wandb\n",
    "import time\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bc4f6f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bc4f6f3",
    "outputId": "ba7ca832-fe13-41cd-8181-ce79efbd5054"
   },
   "outputs": [],
   "source": [
    "#!wget https://github.com/Toloka/WSDMCup2023/raw/main/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bd90ccd",
   "metadata": {
    "id": "8bd90ccd"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"/home/ngoc/data/WSDM2023/\" # change to /content/ if running on GG COLAB\n",
    "VISUALIZATION = False  \n",
    "BATCH_SIZE = 4              \n",
    "NUM_EPOCHS = 100           \n",
    "IMAGE_EMBEDIND_SIZE = 1024     \n",
    "WORD_EMBEDING_SIZE = 300        \n",
    "NUMBER_FC_LAYER = 2           \n",
    "HIDDEN_FC_SIZE = 1024        \n",
    "LEARNING_RATE =0.00001         \n",
    "MODEL = \"VQA\"                      \n",
    "IMAGE_SIZE = 640\n",
    "LOSS = \"MSLE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9337e67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9337e67",
    "outputId": "7128be44-6703-4bcd-bcaa-d99d795798ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c469947",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TRAINING  = True\n",
    "WANDB_LOG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7590fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if WANDB_LOG:\n",
    "    import wandb\n",
    "    wandb.init(project=\"WSDM2023\",\n",
    "            entity=\"ngocdunghuynh\",\n",
    "                config = {\n",
    "                    \"learning_rate\": LEARNING_RATE,\n",
    "                    \"epochs\": NUM_EPOCHS,\n",
    "                    \"batch_size\": BATCH_SIZE,\n",
    "                    \"loss_function\": LOSS,\n",
    "                })\n",
    "    #WandB key: 40448fdd0d7d5dd051f607a1fcdec27cd9b84996\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e5ed92",
   "metadata": {
    "id": "f4e5ed92"
   },
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad0f2df4",
   "metadata": {
    "id": "ad0f2df4"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR  + 'train.csv')\n",
    "TRAIN_IMGS_DIR = DATA_DIR + \"train_imgs/\"\n",
    "#if TEST_TRAINING:\n",
    "#    train = train[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9f0d2b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "0662ee4595f34c9e85327822edbcaa11",
      "df61bdd17d654c7a80a81dc9a18bff10",
      "3c8a38e0890845b1a4ca964b9a576134",
      "dbdd88c4af6b4658beb33a9071a4d465",
      "68dc90642ee144838e183c09f702a365",
      "0d956e2818dd4cb7b1faa3e00e5e6403",
      "5966ae2880be465ab6b0d54173faf8f1",
      "a90d8de2dc50453784df8f1229c293e0",
      "1c0b9157b1e446a9950f6becd39f40de",
      "90ee8d02cde54101ba121921f46ee1a5",
      "9b2c48ea9450446b91a9ce26001be3f6"
     ]
    },
    "id": "e9f0d2b0",
    "outputId": "f84074d4-9851-4bd8-8e7a-f8628b27acbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the training set has been downloaded\n"
     ]
    }
   ],
   "source": [
    "# Download data\n",
    "try:\n",
    "    os.mkdir(TRAIN_IMGS_DIR)\n",
    "    img_paths = Parallel(\n",
    "    n_jobs=100)(delayed(wget.download)(img_url, out= TRAIN_IMGS_DIR) for img_url in tqdm(train.image)\n",
    "    )\n",
    "except:\n",
    "    print(\"the training set has been downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7188dd62",
   "metadata": {
    "id": "7188dd62"
   },
   "outputs": [],
   "source": [
    "#Get the image path from URL\n",
    "train.image  = train.image.apply(lambda x: str(x.split(\"/\")[-1]))\n",
    "train.image  = train.image.apply(lambda x: TRAIN_IMGS_DIR + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58000096",
   "metadata": {
    "id": "58000096"
   },
   "outputs": [],
   "source": [
    "#Spit the original training set into 2 dataframes\n",
    "train_sample = int(len(train)*0.8)\n",
    "df_train = train[:int(len(train)*0.8)]\n",
    "df_val = train[int(len(train)*0.8):]\n",
    "\n",
    "df_train = df_train.reset_index(drop =True)\n",
    "df_val = df_val.reset_index(drop =True)\n",
    "\n",
    "if TEST_TRAINING:\n",
    "    df_train = df_train[:1000*BATCH_SIZE]\n",
    "    df_val =  df_val[:100*BATCH_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1932736b",
   "metadata": {
    "id": "1932736b"
   },
   "source": [
    "## Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "286067fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "286067fe",
    "outputId": "205a88a1-aa58-4beb-de36-3a6e4b73aa2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ngoc/anaconda3/envs/tf/lib/python3.7/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from concurrent.futures import process\n",
    "\n",
    "\n",
    "def create_mask(bb, x):\n",
    "    \"\"\"Creates a mask for the bounding box of same shape as image\"\"\"\n",
    "    cols, rows,*_ = x.shape\n",
    "    Y = np.zeros((cols, rows))\n",
    "    bb = bb.astype(np.int64)\n",
    "    Y[bb[1]:bb[3], bb[0]:bb[2]] = 1.\n",
    "    return Y\n",
    "\n",
    "def mask_to_bb(Y):\n",
    "    \"\"\"Convert mask Y to a bounding box, assumes 0 as background nonzero object\"\"\"\n",
    "    rows , cols = np.nonzero(Y)\n",
    "    if len(cols)==0: \n",
    "        return np.zeros(4, dtype=np.float32)\n",
    "    top_row = np.min(rows)\n",
    "    left_col = np.min(cols)\n",
    "    bottom_row = np.max(rows)\n",
    "    right_col = np.max(cols)\n",
    "    return np.array([left_col, top_row, right_col, bottom_row], dtype=np.int64)\n",
    "\n",
    "def xyxy2xywh(box): #Convert box format [xmin ymin xmax ymax] --> [x center, y center, width, height] and normalize the box\n",
    "    xmin, ymin, xmax, ymax = box\n",
    "    w = xmax - xmin         # Width of the box\n",
    "    h = ymax - ymin         # Height of the box\n",
    "    x_center = (xmin + (w/2))/IMAGE_SIZE \n",
    "    y_center = (ymin + (h/2))/IMAGE_SIZE \n",
    "    new_w = w/ IMAGE_SIZE \n",
    "    new_h = h/IMAGE_SIZE\n",
    "    return np.array([x_center,y_center, new_w, new_h], dtype=np.float32)\n",
    "\n",
    "def resize_image_bb(img_path,bb,sz, question, show_example):\n",
    "    \"\"\"Resize an image and its bounding box and write image to new path\"\"\"\n",
    "    img = cv2.cvtColor(cv2.imread(str(img_path)), cv2.COLOR_BGR2RGB)\n",
    "    mask = create_mask(bb, img)\n",
    "    if show_example:\n",
    "        result = img.copy()\n",
    "        cv2.rectangle(result, (bb[0], bb[1]), (bb[2], bb[3]), (0, 255, 0), 5)\n",
    "        plt.figure()\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.xlabel(f\"{question} \\n bb: {bb}\")\n",
    "        plt.imshow(result)\n",
    "        plt.show\n",
    "    \n",
    "    img_resized = cv2.resize(img, (sz, sz))\n",
    "    mask_resized = cv2.resize(mask, (sz, sz))\n",
    "    bb_resized = mask_to_bb(mask_resized)\n",
    "    x_center,y_center, new_w, new_h = xyxy2xywh(bb_resized)\n",
    "    processed_box = np.array([x_center,y_center, new_w, new_h], dtype=np.float32)\n",
    "    if show_example:\n",
    "\n",
    "        new_result = img_resized.copy()\n",
    "        cv2.rectangle(new_result, (bb_resized[0], bb_resized[1]), (bb_resized[2], bb_resized[3]), (0, 255, 0), 5)\n",
    "        plt.figure()\n",
    "        plt.title(\"Resized Image\")\n",
    "        plt.imshow(new_result)\n",
    "        plt.xlabel(f\"{question} \\n bb: {bb_resized} \\n processed_box: {processed_box}\")\n",
    "        plt.show\n",
    "    return np.array(img_resized, dtype = np.float32), bb_resized, processed_box\n",
    "\n",
    "\n",
    "\n",
    "def xywh2xyxy(box): #Convert box format [x center, y center, width, height]  --> [xmin ymin xmax ymax] and normalize the box\n",
    "    x_center,y_center, new_w, new_h = box*IMAGE_SIZE\n",
    "    xmin = x_center - (new_w/2)\n",
    "    ymin = y_center - (new_h/2)\n",
    "    xmax = x_center + (new_w/2)\n",
    "    ymax = y_center + (new_h/2)\n",
    "    return np.array([xmin, ymin, xmax, ymax], dtype=np.int64)\n",
    "\n",
    "def image_box_processing(image_path, box, question = None, show_example = True):\n",
    "        img_resized, bb_resized , processed_box = resize_image_bb(image_path, box,IMAGE_SIZE, question, show_example)\n",
    "        return img_resized, bb_resized, processed_box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e89c5d",
   "metadata": {
    "id": "25e89c5d"
   },
   "source": [
    "## Question Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "689a71ce",
   "metadata": {
    "id": "689a71ce"
   },
   "outputs": [],
   "source": [
    "class Text_Dict:\n",
    "\n",
    "    \"\"\"Aim to convert index to word or word to index\"\"\"\n",
    "\n",
    "    def __init__(self, vocab):\n",
    "        \n",
    "        self.word_list = vocab\n",
    "        self.word2idx_dict = {w:n_w for n_w, w in enumerate(self.word_list)}\n",
    "        self.vocab_size = len(self.word_list)\n",
    "        self.unk2idx = self.word2idx_dict['<unk>'] if '<unk>' in self.word2idx_dict else None\n",
    "\n",
    "    def idx2word(self, n_w):\n",
    "\n",
    "        return self.word_list[n_w]\n",
    "\n",
    "    def word2idx(self, w):\n",
    "        if w in self.word2idx_dict:\n",
    "            return self.word2idx_dict[w]\n",
    "        elif self.unk2idx is not None:\n",
    "            return self.unk2idx\n",
    "        else:\n",
    "            raise ValueError(f'word {w} not in dictionary (while dictionary does not contain <unk>)')\n",
    "\n",
    "    def tokenize_and_index(self, sentence):\n",
    "        inds = [self.word2idx(w) for w in tokenize(sentence)]\n",
    "\n",
    "        return inds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ce2de3",
   "metadata": {
    "id": "e6ce2de3"
   },
   "source": [
    "## Data Pipeline Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7a28dfe",
   "metadata": {
    "id": "d7a28dfe"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, dataset, question_vocab, batch_size=32):\n",
    "        'Initialization'\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.question_vocab = question_vocab\n",
    "        self.question_dict = Text_Dict(self.question_vocab)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.dataset) / self.batch_size))\n",
    "\n",
    "\n",
    "    def make_vocab_questions(self, questions):\n",
    "        \"\"\"Make dictionary for questions and save them into text file.\"\"\"\n",
    "\n",
    "        print(\"- Creating a vocab list for questions\")\n",
    "        vocab_set = set()\n",
    "        SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n",
    "        question_length = []\n",
    "\n",
    "        set_question_length = [None]*len(questions)\n",
    "        for iquestion, question in enumerate(questions):\n",
    "            words = SENTENCE_SPLIT_REGEX.split(question.lower())\n",
    "            words = [w.strip() for w in words if len(w.strip()) > 0]\n",
    "            vocab_set.update(words)\n",
    "            set_question_length[iquestion] = len(words)\n",
    "        question_length += set_question_length\n",
    "\n",
    "        vocab_list = list(vocab_set)\n",
    "        vocab_list.sort()\n",
    "        vocab_list.insert(0, '')\n",
    "        vocab_list.insert(1, '')\n",
    "        print(f'    + The size of Question vocabbulary {len(vocab_list)}.')\n",
    "        print(f'    + Maximum length of question: {np.max(question_length)}')\n",
    "        return vocab_list\n",
    "\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "\n",
    "        \"\"\"Split string lines into lists\"\"\"\n",
    "        \n",
    "        SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n",
    "        tokens = SENTENCE_SPLIT_REGEX.split(sentence.lower())\n",
    "        tokens = [t.strip() for t in tokens if len(t.strip()) > 0]\n",
    "        return tokens\n",
    "\n",
    "    def question_processing(self, question):\n",
    "        question_token = self.tokenize(question)\n",
    "        question_2_idx = [self.question_dict.word2idx(w) for w in question_token ]\n",
    "        return  question_2_idx\n",
    "    \n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.dataset))\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        batch_data = [self.dataset.iloc[k] for k in indexes]\n",
    "        images, questions, boxes = self.__data_generation(batch_data)\n",
    "        return [{\n",
    "            'input_image': np.array(images, dtype =np.float32),\n",
    "            'input_question': np.array(questions, dtype =int)},\n",
    "            np.array(boxes, dtype =np.float32)]\n",
    "\n",
    "    def __data_generation(self, batch_data):\n",
    "        images = []\n",
    "        questions = []\n",
    "        boxes = []\n",
    "        question_len = []\n",
    "        for image_path, width, hieght, left, top, right, bottom, question in batch_data:\n",
    "            box = np.array([left, top, right, bottom],dtype = np.float32)\n",
    "            resized_image, resized_box, processed_box = image_box_processing(image_path, box, question = None, show_example = False)\n",
    "\n",
    "            question = self.question_processing(question)\n",
    "            question_len.append(len(question))\n",
    "\n",
    "            questions.append(question)\n",
    "            boxes.append(processed_box)\n",
    "            images.append(resized_image/225)\n",
    "        questions = pad_sequences(questions, maxlen = max(question_len), dtype='float32', value=0, padding='pre')\n",
    "        return images, questions, boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7838111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab_questions(questions):\n",
    "        \"\"\"Make dictionary for questions and save them into text file.\"\"\"\n",
    "\n",
    "        print(\"- Creating a vocab list for questions\")\n",
    "        vocab_set = set()\n",
    "        SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n",
    "        question_length = []\n",
    "\n",
    "        set_question_length = [None]*len(questions)\n",
    "        for iquestion, question in enumerate(questions):\n",
    "            words = SENTENCE_SPLIT_REGEX.split(question.lower())\n",
    "            words = [w.strip() for w in words if len(w.strip()) > 0]\n",
    "            vocab_set.update(words)\n",
    "            set_question_length[iquestion] = len(words)\n",
    "        question_length += set_question_length\n",
    "\n",
    "        vocab_list = list(vocab_set)\n",
    "        vocab_list.sort()\n",
    "        vocab_list.insert(0, '')\n",
    "        vocab_list.insert(1, '')\n",
    "        print(f'    + The size of Question vocabbulary {len(vocab_list)}.')\n",
    "        print(f'    + Maximum length of question: {np.max(question_length)}')\n",
    "        return vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aebd4aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Creating a vocab list for questions\n",
      "    + The size of Question vocabbulary 7494.\n",
      "    + Maximum length of question: 41\n"
     ]
    }
   ],
   "source": [
    "question_vocab = make_vocab_questions(train[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de26939",
   "metadata": {
    "id": "7de26939"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06081a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(y_true, y_pred, xyxy = False):\n",
    "    # iou as metric for bounding box regression\n",
    "    \n",
    "    if not xyxy:\n",
    "        # convert input to be as [x1, y1, x2, y2]\n",
    "        y_true = y_true*IMAGE_SIZE\n",
    "        y_true_xmin, y_true_ymin  = y_true[:,0] - y_true[:,2]/2, y_true[:,1] - y_true[:,3]/2\n",
    "        y_true_xmax, y_true_ymax  = y_true[:,0] + y_true[:,2]/2, y_true[:,1] + y_true[:,3]/2\n",
    "        y_true = tf.stack([y_true_xmin, y_true_ymin, y_true_xmax, y_true_ymax], 1)\n",
    "    \n",
    "        # convert predictions to be as [x1, y1, x2, y2]\n",
    "        y_pred = y_pred*IMAGE_SIZE\n",
    "        y_pred_xmin, y_pred_ymin  = y_pred[:,0] - y_pred[:,2]/2, y_pred[:,1] - y_pred[:,3]/2\n",
    "        y_pred_xmax, y_pred_ymax  = y_pred[:,0] + y_pred[:,2]/2, y_pred[:,1] + y_pred[:,3]/2\n",
    "        y_pred = tf.stack([y_pred_xmin, y_pred_ymin, y_pred_xmax, y_pred_ymax], 1)\n",
    "    # AOG = Area of Groundtruth box\n",
    "    AoG = K.abs(K.transpose(y_true)[2] - K.transpose(y_true)[0] + 1) * K.abs(K.transpose(y_true)[3] - K.transpose(y_true)[1] + 1)\n",
    "    # AOP = Area of Predicted box\n",
    "    AoP = K.abs(K.transpose(y_pred)[2] - K.transpose(y_pred)[0] + 1) * K.abs(K.transpose(y_pred)[3] - K.transpose(y_pred)[1] + 1)\n",
    "\n",
    "    # overlaps are the co-ordinates of intersection box\n",
    "    overlap_0 = K.maximum(K.transpose(y_true)[0], K.transpose(y_pred)[0])\n",
    "    overlap_1 = K.maximum(K.transpose(y_true)[1], K.transpose(y_pred)[1])\n",
    "    overlap_2 = K.minimum(K.transpose(y_true)[2], K.transpose(y_pred)[2])\n",
    "    overlap_3 = K.minimum(K.transpose(y_true)[3], K.transpose(y_pred)[3])\n",
    "\n",
    "    # intersection area\n",
    "    intersection = (overlap_2 - overlap_0 + 1) * (overlap_3 - overlap_1 + 1)\n",
    "\n",
    "    # area of union of both boxes\n",
    "    union = AoG + AoP - intersection\n",
    "    \n",
    "    # iou calculation\n",
    "    iou = intersection / union\n",
    "\n",
    "    # bounding values of iou to (0,1)\n",
    "    iou = K.clip(iou, 0.0 + K.epsilon(), 1.0 - K.epsilon())\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da5a1869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prediction(img, img_dix, pred_bb, gt_bb, question, epoch, mode, xyxy = False):\n",
    "    cv2.imwrite(f'./saved_images/{mode}_img_{img_dix}.jpg', img)\n",
    "    img = cv2.cvtColor(cv2.imread(f'./saved_images/{mode}_img_{img_dix}.jpg'), cv2.COLOR_BGR2RGB)\n",
    "    result = img.copy()\n",
    "    cv2.rectangle(result, (int(gt_bb[0]), int(gt_bb[1])), (int(gt_bb[2]), int(gt_bb[3])), (0, 255, 0), 3)\n",
    "    cv2.rectangle(result, (int(pred_bb[0]), int(pred_bb[1])), (int(pred_bb[2]), int(pred_bb[3])), (255, 0, 0), 3)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(question)\n",
    "    plt.savefig(f\"./saved_images/{mode}_img_{img_dix}_epoch_{epoch}.jpg\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5081037",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=[\"epoch\", \"dataset\", \"image\", \"iou\"]\n",
    "train_table = wandb.Table(columns=columns)\n",
    "val_table = wandb.Table(columns=columns)\n",
    "class Log_Img_Callback(keras.callbacks.Callback):\n",
    "    \n",
    "    #SAVE PREDICTIONS ON WANDB\n",
    "    def __init__(self, train_generator,val_generator):\n",
    "        super().__init__()\n",
    "        self.train_samples = train_generator[0]\n",
    "        self.val_samples = val_generator[0]\n",
    "        self.train_generator = train_generator\n",
    "        \n",
    "    def iou(self, box_predicted, box_truth):\n",
    "        # get (x, y) coordinates of intersection of bounding boxes\n",
    "        top_x_intersect = max(box_predicted[0], box_truth[0])\n",
    "        top_y_intersect = max(box_predicted[1], box_truth[1])\n",
    "        bottom_x_intersect = min(box_predicted[2], box_truth[2])\n",
    "        bottom_y_intersect = min(box_predicted[3], box_truth[3])\n",
    "\n",
    "        # calculate area of the intersection bb (bounding box)\n",
    "        intersection_area = max(0, bottom_x_intersect - top_x_intersect + 1) * max(\n",
    "            0, bottom_y_intersect - top_y_intersect + 1\n",
    "        )\n",
    "\n",
    "        # calculate area of the prediction bb and ground-truth bb\n",
    "        box_predicted_area = (box_predicted[2] - box_predicted[0] + 1) * (\n",
    "            box_predicted[3] - box_predicted[1] + 1\n",
    "        )\n",
    "        box_truth_area = (box_truth[2] - box_truth[0] + 1) * (\n",
    "            box_truth[3] - box_truth[1] + 1\n",
    "        )\n",
    "\n",
    "        # calculate intersection over union by taking intersection\n",
    "        # area and dividing it by the sum of predicted bb and ground truth\n",
    "        # bb areas subtracted by  the interesection area\n",
    "\n",
    "        # return ioU\n",
    "        return intersection_area / float(\n",
    "            box_predicted_area + box_truth_area - intersection_area\n",
    "        )\n",
    "    \n",
    "    def xywh_to_xyxy(self, bboxes):\n",
    "        bboxes = bboxes*IMAGE_SIZE\n",
    "        bboxes[:,0], bboxes[:,2] = bboxes[:,0] - bboxes[:,2]/2,bboxes[:,0] + bboxes[:,2]/2\n",
    "        bboxes[:,1], bboxes[:,3] = bboxes[:,1] - bboxes[:,3]/2, bboxes[:,1] + bboxes[:,3]/2\n",
    "        return bboxes\n",
    "        \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(epoch)\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            train_input, train_output = self.train_samples\n",
    "            train_predictions = self.model.predict(train_input, verbose = 0)\n",
    "            train_predictions =self.xywh_to_xyxy(train_predictions)\n",
    "            train_output = self.xywh_to_xyxy(train_output[\"output\"])\n",
    "            for i in range(len(train_predictions)):\n",
    "                iou = self.iou(train_predictions[i], train_output[i])\n",
    "                img = train_input[\"input_image\"][i]*225\n",
    "                text = \"\"\n",
    "                for idx in train_input[\"input_question\"][i]:\n",
    "                    text += self.train_generator.question_dict.idx2word(idx)\n",
    "                    text += \" \"\n",
    "                save_prediction(img, i, train_predictions[i], train_output[i], text, epoch+1, \"train\")\n",
    "                train_table.add_data(epoch+1, \"train\", wandb.Image(f\"./saved_images/train_img_{i}_epoch_{epoch+1}.jpg\"), iou)\n",
    "            \n",
    "            val_input, val_output = self.val_samples\n",
    "            val_predictions = self.model.predict(val_input, verbose = 0)\n",
    "            val_predictions =self.xywh_to_xyxy(val_predictions)\n",
    "            val_output = self.xywh_to_xyxy(val_output[\"output\"])\n",
    "            for i in range(len(val_predictions)):\n",
    "                iou = self.iou(val_predictions[i], val_output[i])\n",
    "                img = val_input[\"input_image\"][i]*225\n",
    "                text = \"\"\n",
    "                for idx in val_input[\"input_question\"][i]:\n",
    "                    text += self.train_generator.question_dict.idx2word(idx)\n",
    "                    text += \" \"\n",
    "                save_prediction(img, i, val_predictions[i], val_output[i], text, epoch+1, \"val\")\n",
    "                val_table.add_data(epoch+1, \"val\", wandb.Image(f\"./saved_images/val_img_{i}_epoch_{epoch+1}.jpg\"), iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5adcc3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_size, img_size):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        # Define VGG 16\n",
    "        self.vgg_net = VGG16(weights='imagenet',include_top=False, input_shape=(img_size, img_size, 3))\n",
    "        self.vgg_net.trainable = False\n",
    "        self.dense = layers.Dense(embedding_size, activation = \"relu\")\n",
    "    \n",
    "    def call(self, images):\n",
    "        # batch*img_size*img_size*3\n",
    "        image_tensor = tf.convert_to_tensor(images)\n",
    "\n",
    "        # batch*20*20*512\n",
    "        image_feature = self.vgg_net(image_tensor)\n",
    "\n",
    "        # batch*400*512\n",
    "        #image_feature = layers.Flatten()(image_feature)\n",
    "        image_feature = tf.reshape(image_feature , [-1,image_feature.shape[2]*image_feature.shape[1] , image_feature.shape[3]])\n",
    "        \n",
    "        # batch*400*embedding_size\n",
    "\n",
    "        image_feature = self.dense(image_feature)\n",
    "        return image_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9a69fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, question_vocab_size, question_embedding_size, embedding_size, rnn_hidden_size):\n",
    "        super(QuestionEncoder, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "\n",
    "        #Define Question Embeding\n",
    "        self.word2vec  = layers.Embedding(input_dim=question_vocab_size, output_dim = question_embedding_size)\n",
    "\n",
    "        # Define LSTM\n",
    "        self.lstm = tf.keras.layers.LSTM(rnn_hidden_size)\n",
    "\n",
    "        #Define dense output layer\n",
    "        self.dense = layers.Dense(embedding_size, activation = \"tanh\")\n",
    "\n",
    "    def call(self, questions):\n",
    "        # batch*question_len\n",
    "\n",
    "        question_tensor = tf.convert_to_tensor(questions)\n",
    "\n",
    "        # batch*question_len*question_embedding_size\n",
    "        question_vec = self.word2vec(question_tensor)\n",
    "        \n",
    "        # batch*rnn_hidden_size\n",
    "        question_feature  =  self.lstm(question_vec)\n",
    "        \n",
    "        #question_feature = self.dense(question_feature)\n",
    "        return question_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40c92d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        self.image_feature_convert = layers.Dense(output_dim, activation = 'tanh')\n",
    "        self.question_feature_convert = layers.Dense(output_dim, activation = 'tanh')\n",
    "    \n",
    "    def call(self, image_features, question_features):\n",
    "        #batch*400*embedding_size\n",
    "        hi = self.image_feature_convert(image_features)\n",
    "        #batch*embedding_size\n",
    "        hq = self.question_feature_convert(question_features)\n",
    "        #batch*1*embedding_size\n",
    "        hq = tf.expand_dims(hq, axis = 1)\n",
    "\n",
    "        #batch*400*embedding_size\n",
    "        ha  = tf.tanh(hi + hq)\n",
    "        #ha = layers.Dropout(0.5)(ha)\n",
    "        ha = layers.Dense(1)(ha)\n",
    "        ha = tf.reshape(ha , [-1,ha.shape[1]])\n",
    "        pi = tf.nn.softmax(ha)\n",
    "        pi = tf.expand_dims(pi , axis = -1)\n",
    "        att_layer = tf.reduce_sum(pi * image_features , axis = 1)\n",
    "        u = att_layer + question_features\n",
    "        return  tf.convert_to_tensor(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d622b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQA(tf.keras.Model):\n",
    "    def __init__(self, embedding_size, \n",
    "                img_size, \n",
    "                question_vocab_size, \n",
    "                question_embedding_size,\n",
    "                rnn_hidden_size,\n",
    "                atention_out_dim):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder(embedding_size, img_size)\n",
    "        self.question_encoder = QuestionEncoder(\n",
    "                                    question_vocab_size = question_vocab_size,\n",
    "                                    question_embedding_size = question_embedding_size,\n",
    "                                    embedding_size = embedding_size,\n",
    "                                    rnn_hidden_size = rnn_hidden_size\n",
    "                                )\n",
    "        \n",
    "        self.attention = Attention(atention_out_dim)\n",
    "        self.output_layer = layers.Dense(4, activation='sigmoid', name=\"output\")\n",
    "\n",
    "    def call(self, data):\n",
    "\n",
    "        images, questions = data[\"input_image\"], data[\"input_question\"]\n",
    "\n",
    "        image_features = self.image_encoder(images)\n",
    "\n",
    "        question_features = self.question_encoder(questions)\n",
    "        u = self.attention(image_features, question_features)\n",
    "        output = self.output_layer(u)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b7334fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch < 20:\n",
    "        return lr\n",
    "    elif epoch < 40:\n",
    "        return lr/10\n",
    "    elif epoch < 80:\n",
    "        return lr/100\n",
    "    elif epoch < 80:\n",
    "        return lr/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b5eb08",
   "metadata": {},
   "source": [
    "# Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9c04f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2e9c04f",
    "outputId": "5d8f3bd8-44ca-4c59-c77f-4f4f9cd9bf72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ngoc/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/data/ops/structured_function.py:265: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  \"Even though the `tf.config.experimental_run_functions_eagerly` \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 384s 382ms/step - loss: 0.0190 - iou: 0.1177 - mae: 0.1494 - root_mean_squared_error: 0.1940 - val_loss: 0.0180 - val_iou: 0.1355 - val_mae: 0.1470 - val_root_mean_squared_error: 0.1870 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 382s 382ms/step - loss: 0.0165 - iou: 0.1185 - mae: 0.1382 - root_mean_squared_error: 0.1817 - val_loss: 0.0175 - val_iou: 0.1435 - val_mae: 0.1451 - val_root_mean_squared_error: 0.1857 - lr: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 382s 382ms/step - loss: 0.0146 - iou: 0.1242 - mae: 0.1284 - root_mean_squared_error: 0.1714 - val_loss: 0.0183 - val_iou: 0.1253 - val_mae: 0.1455 - val_root_mean_squared_error: 0.1894 - lr: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 382s 382ms/step - loss: 0.0128 - iou: 0.1338 - mae: 0.1189 - root_mean_squared_error: 0.1602 - val_loss: 0.0192 - val_iou: 0.1176 - val_mae: 0.1480 - val_root_mean_squared_error: 0.1933 - lr: 0.0010\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 382s 382ms/step - loss: 0.0108 - iou: 0.1481 - mae: 0.1076 - root_mean_squared_error: 0.1471 - val_loss: 0.0193 - val_iou: 0.1246 - val_mae: 0.1501 - val_root_mean_squared_error: 0.1934 - lr: 0.0010\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - ETA: 0s - loss: 0.0092 - iou: 0.1689 - mae: 0.0984 - root_mean_squared_error: 0.1360"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.OneDeviceStrategy(device=\"/GPU:0\")\n",
    "with strategy.scope():\n",
    "    #Data Generation\n",
    "    train_generator = DataGenerator(df_train,question_vocab, BATCH_SIZE)\n",
    "    val_generator = DataGenerator(df_val,question_vocab, BATCH_SIZE)\n",
    "    \n",
    "    #VGG16 base\n",
    "    conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(640, 640, 3))\n",
    "    #Freeze the weight\n",
    "    conv_base.trainable = False\n",
    "    \n",
    "    #Get vocal size of questrion\n",
    "    vocab_size = train_generator.question_dict.vocab_size\n",
    "    \n",
    "    #Create VQA model\n",
    "    VQA_model = VQA(\n",
    "        embedding_size = 1024,\n",
    "        img_size=640,\n",
    "        question_embedding_size= 300,\n",
    "        question_vocab_size = train_generator.question_dict.vocab_size,\n",
    "        rnn_hidden_size=1024,\n",
    "        atention_out_dim = 512,\n",
    "    \n",
    "    )\n",
    "    \n",
    "    VQA_model.compile(keras.optimizers.Adam(learning_rate=0.001),\n",
    "                      loss=tf.keras.losses.MeanSquaredLogarithmicError(), metrics = [iou, 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_iou', patience=3) \n",
    "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose = 1)\n",
    "    \n",
    "    if WANDB_LOG:\n",
    "        wandb_img_callback = Log_Img_Callback(train_generator, val_generator)\n",
    "        wandb_loss_log = WandbCallback(monitor=\"val_iou\", mode=\"max\")\n",
    "        callbacks = [wandb_img_callback, wandb_loss_log, early_stop, lr_schedule]\n",
    "    else:\n",
    "        callbacks = [ lr_schedule]\n",
    "        \n",
    "        \n",
    "        \n",
    "    history = VQA_model.fit(\n",
    "        train_generator,\n",
    "        epochs=100,\n",
    "        callbacks = callbacks,\n",
    "        verbose = 1,\n",
    "        validation_data = val_generator,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b4442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb53c49",
   "metadata": {
    "id": "cbb53c49"
   },
   "outputs": [],
   "source": [
    "if WANDB_LOG:\n",
    "    wandb.log({\"train example\": train_table})\n",
    "    wandb.log({\"val example\": val_table})\n",
    "    #os.system('rm -rf saved_images')\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35d236f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a27de88aa722789546af8c76440849ea85409493cc9ee2de95bd6e6783593fb"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0662ee4595f34c9e85327822edbcaa11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_df61bdd17d654c7a80a81dc9a18bff10",
       "IPY_MODEL_3c8a38e0890845b1a4ca964b9a576134",
       "IPY_MODEL_dbdd88c4af6b4658beb33a9071a4d465"
      ],
      "layout": "IPY_MODEL_68dc90642ee144838e183c09f702a365"
     }
    },
    "0d956e2818dd4cb7b1faa3e00e5e6403": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c0b9157b1e446a9950f6becd39f40de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3c8a38e0890845b1a4ca964b9a576134": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a90d8de2dc50453784df8f1229c293e0",
      "max": 5000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1c0b9157b1e446a9950f6becd39f40de",
      "value": 5000
     }
    },
    "5966ae2880be465ab6b0d54173faf8f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "68dc90642ee144838e183c09f702a365": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90ee8d02cde54101ba121921f46ee1a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b2c48ea9450446b91a9ce26001be3f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a90d8de2dc50453784df8f1229c293e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbdd88c4af6b4658beb33a9071a4d465": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90ee8d02cde54101ba121921f46ee1a5",
      "placeholder": "​",
      "style": "IPY_MODEL_9b2c48ea9450446b91a9ce26001be3f6",
      "value": " 5000/5000 [01:05&lt;00:00, 138.96it/s]"
     }
    },
    "df61bdd17d654c7a80a81dc9a18bff10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d956e2818dd4cb7b1faa3e00e5e6403",
      "placeholder": "​",
      "style": "IPY_MODEL_5966ae2880be465ab6b0d54173faf8f1",
      "value": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
