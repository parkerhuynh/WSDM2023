{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import uuid\n",
    "import tqdm\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import copy\n",
    "# import h5py\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import scipy.io as sio\n",
    "import torch.utils.data as data\n",
    "from collections import OrderedDict\n",
    "sys.path.append('.')\n",
    "import operator\n",
    "import pickle\n",
    "import argparse\n",
    "import collections\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "# from transformers import BertTokenizer,BertModel\n",
    "from transforms import letterbox, random_affine\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    def __init__(self, unique_id, text_a, text_b):\n",
    "        self.unique_id = unique_id\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples(input_line, unique_id):\n",
    "    \"\"\"Read a list of `InputExample`s from an input file.\"\"\"\n",
    "    examples = []\n",
    "    # unique_id = 0\n",
    "    line = input_line #reader.readline()\n",
    "    # if not line:\n",
    "    #     break\n",
    "    line = line.strip()\n",
    "    text_a = None\n",
    "    text_b = None\n",
    "    m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n",
    "    if m is None:\n",
    "        text_a = line\n",
    "    else:\n",
    "        text_a = m.group(1)\n",
    "        text_b = m.group(2)\n",
    "    examples.append(\n",
    "        InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b))\n",
    "    # unique_id += 1\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(df, phase):\n",
    "    \n",
    "    \"\"\"Create a data list to store all raw data simples\"\"\"\n",
    "    data = []\n",
    "    for idx in range(len(df)):\n",
    "        img_path, W, H, l, t, r,b, question = df.iloc[idx]\n",
    "        \n",
    "        dic  = {\n",
    "            \"question\": question\n",
    "        }\n",
    "        if phase != \"test\":\n",
    "            \n",
    "            img_path = '/home/ngoc/data/WSDM2023/train_imgs/' + img_path.split('/')[-1]\n",
    "            dic[\"img_path\"] = img_path\n",
    "            dic[\"bb\"] = np.array([W, H,l,t,r,b], dtype=float)\n",
    "        else:\n",
    "            image_path = '/home/ngoc/data/WSDM2023/test_imgs/' + img_path.split('/')[-1]\n",
    "            dic[\"img_path\"] = img_path\n",
    "        data.append(dic)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples(input_line, unique_id):\n",
    "    \"\"\"Read a list of `InputExample`s from an input file.\"\"\"\n",
    "    examples = []\n",
    "    # unique_id = 0\n",
    "    line = input_line #reader.readline()\n",
    "    # if not line:\n",
    "    #     break\n",
    "    line = line.strip()\n",
    "    text_a = None\n",
    "    text_b = None\n",
    "    m = re.match(r\"^(.*) \\|\\|\\| (.*)$\", line)\n",
    "    if m is None:\n",
    "        text_a = line\n",
    "    else:\n",
    "        text_a = m.group(1)\n",
    "        text_b = m.group(2)\n",
    "    examples.append(\n",
    "        InputExample(unique_id=unique_id, text_a=text_a, text_b=text_b))\n",
    "    # unique_id += 1\n",
    "    return examples\n",
    "\n",
    "## Bert text encoding\n",
    "class InputExample(object):\n",
    "    def __init__(self, unique_id, text_a, text_b):\n",
    "        self.unique_id = unique_id\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "    def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids):\n",
    "        self.unique_id = unique_id\n",
    "        self.tokens = tokens\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.input_type_ids = input_type_ids\n",
    "\n",
    "def convert_examples_to_features(examples, seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "        if tokens_b:\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(seq_length - 2)]\n",
    "        tokens = []\n",
    "        input_type_ids = []\n",
    "        tokens.append(\"[CLS]\")\n",
    "        input_type_ids.append(0)\n",
    "        for token in tokens_a:\n",
    "            tokens.append(token)\n",
    "            input_type_ids.append(0)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        input_type_ids.append(0)\n",
    "\n",
    "        if tokens_b:\n",
    "            for token in tokens_b:\n",
    "                tokens.append(token)\n",
    "                input_type_ids.append(1)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            input_type_ids.append(1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            input_type_ids.append(0)\n",
    "\n",
    "        assert len(input_ids) == seq_length\n",
    "        assert len(input_mask) == seq_length\n",
    "        assert len(input_type_ids) == seq_length\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                unique_id=example.unique_id,\n",
    "                tokens=tokens,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                input_type_ids=input_type_ids))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSDMDataset(data.Dataset):\n",
    "    def __init__(self, dataset, phase, img_size, max_query_len = 40, transform = None, augment = False, bert_model = 'bert-base-uncased'):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.phase = phase\n",
    "        self.img_size = img_size\n",
    "        self.max_query_len = max_query_len\n",
    "        self.bert_model = bert_model\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        #Visual Processing\n",
    "        img_path = self.dataset[idx]['img_path']\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[0], img.shape[1]\n",
    "        mask = np.zeros_like(img)\n",
    "        img, mask, ratio, dw, dh = letterbox(img, mask, self.img_size[0])\n",
    "        \n",
    "        #Bounding Box processing\n",
    "        bbox = self.dataset[idx]['bb'][2:] #[x_min, y_min, x_max, y_max]\n",
    "        bbox[0], bbox[2] = bbox[0]*ratio+dw, bbox[2]*ratio+dw\n",
    "        bbox[1], bbox[3] = bbox[1]*ratio+dh, bbox[3]*ratio+dh\n",
    "\n",
    "        #Question processing\n",
    "        question = self.dataset[idx]['question']\n",
    "        examples = read_examples(question, idx)\n",
    "        features = convert_examples_to_features(\n",
    "                examples=examples, seq_length=self.max_query_len, tokenizer=self.tokenizer)\n",
    "        word_id = features[0].input_ids\n",
    "        word_mask = features[0].input_mask\n",
    "\n",
    "        return img, mask,  np.array(word_id, dtype=int),  np.array(word_mask, dtype=int), np.array(bbox, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_loader(data_dir):\n",
    "    train = pd.read_csv(data_dir + 'train.csv')\n",
    "    df_test = pd.read_csv(data_dir + 'test_public.csv')[:100]\n",
    "    train_length = int(len(train)*0.8)\n",
    "    df_train = train[:train_length]\n",
    "    df_valid = train[train_length:]\n",
    "\n",
    "    df_train = processing(df_train, 'train')\n",
    "    df_valid = processing(df_valid, 'val')\n",
    "    df_test = processing(df_test, 'test')\n",
    "    train_dataset = WSDMDataset(df_train, 'train', img_size = (640, 640))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=1,\n",
    "            shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "data_dir = '/home/ngoc/data/WSDM2023/'\n",
    "train = data_loader(data_dir)\n",
    "a = next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb674114faabae68cf36104dc7fe3e83bd3adcaaa054f3409c3c57bd6a2f8498"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
